# -*- coding: utf-8 -*-
"""Gender & Profession

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19LVoKYo6LlD-8gKdvrbGewF8p_XFOKUf
"""

# Installing the Hugging Face "transformers" library (used for BERT)
!pip install transformers torch

from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Loading a pre-trained English BERT model
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# Function: takes a word, returns its "embedding" (numeric meaning vector)
def get_embedding(word):
    tokens = tokenizer(word, return_tensors="pt")
    with torch.no_grad():                 # Don't train; just read BERT's knowledge
        outputs = model(**tokens)
    # Average all token embeddings into one vector for the word
    return outputs.last_hidden_state.mean(dim=1).numpy()

# Words representing gender
male_words = ["man", "he", "male"]
female_words = ["woman", "she", "female"]

# Profession words
professions = ["doctor", "nurse", "engineer", "Teacher", "chef", "writer", "artist"]

# Get average vectors for male and female groups
emb_male = np.mean([get_embedding(w) for w in male_words], axis=0)
emb_female = np.mean([get_embedding(w) for w in female_words], axis=0)

# Compare each profession with male and female
for job in professions:
    emb_job = get_embedding(job)
    sim_male = cosine_similarity(emb_job, emb_male)[0][0]
    sim_female = cosine_similarity(emb_job, emb_female)[0][0]

    # Decide which side it's closer to
    if sim_male > sim_female:
        bias = "Male-biased"
    elif sim_female > sim_male:
        bias = "Female-biased"
    else:
        bias = "Neutral"

    print(f"{job.capitalize():10s} â†’ Male Sim: {sim_male:.3f} | Female Sim: {sim_female:.3f} | Bias: {bias}")